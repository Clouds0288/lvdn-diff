{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ad9f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch_geometric.nn import GINEConv\n",
    "\n",
    "# --- 1. Sinusoidal Time Embeddings ---\n",
    "class SinusoidalPositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# --- 2. Universal GNN Backbone ---\n",
    "class UniversalGNN(nn.Module):\n",
    "    def __init__(self, in_channels=7, hidden_dim=64, time_dim=32, edge_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Node Projection\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Edge Projection\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Time Projection\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # GNN Layers\n",
    "        self.conv1 = GINEConv(\n",
    "            nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        )\n",
    "        self.conv2 = GINEConv(\n",
    "            nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        )\n",
    "        self.conv3 = GINEConv(\n",
    "            nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        )\n",
    "        \n",
    "        # Output Head\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, time_emb, batch):\n",
    "        h = self.node_mlp(x)\n",
    "        edge_feat = self.edge_mlp(edge_attr)\n",
    "        \n",
    "        if time_emb is not None:\n",
    "            t_feat = self.time_mlp(time_emb)\n",
    "            if batch is not None:\n",
    "                h = h + t_feat[batch] # Broadcast\n",
    "            else:\n",
    "                h = h + t_feat\n",
    "\n",
    "        h = self.conv1(h, edge_index, edge_attr=edge_feat)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv2(h, edge_index, edge_attr=edge_feat)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv3(h, edge_index, edge_attr=edge_feat)\n",
    "        h = F.silu(h)\n",
    "\n",
    "        return self.final_mlp(h)\n",
    "\n",
    "# --- 3. Diffusion Scheduler ---\n",
    "class DiffusionScheduler(nn.Module):\n",
    "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.0 - self.alphas_cumprod))\n",
    "\n",
    "    def add_noise(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        # Broadcast logic handled by PyTorch tensors if dimensions align\n",
    "        # Reshaping for safety: [Batch/Nodes, 1]\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].view(-1, 1)\n",
    "        sqrt_om_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)\n",
    "        \n",
    "        x_noisy = sqrt_alpha_t * x_start + sqrt_om_alpha_t * noise\n",
    "        return x_noisy, noise\n",
    "\n",
    "# --- 4. M2 Wrapper (Regressor) ---\n",
    "class M2_GNN_Regressor(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_emb_layer = SinusoidalPositionalEmbeddings(dim=32)\n",
    "        self.backbone = UniversalGNN(in_channels=7, hidden_dim=hidden_dim, time_dim=32)\n",
    "\n",
    "    def forward(self, condition, edge_index, edge_attr, batch):\n",
    "        N = condition.shape[0]\n",
    "        device = condition.device\n",
    "        \n",
    "        # Placeholder for State (Ch 0)\n",
    "        x_state_placeholder = torch.zeros((N, 1), device=device)\n",
    "        x_in = torch.cat([x_state_placeholder, condition], dim=-1)\n",
    "        \n",
    "        # Static Time (t=0)\n",
    "        if batch is not None:\n",
    "            batch_size = batch.max().item() + 1\n",
    "            t = torch.zeros((batch_size,), device=device, dtype=torch.long)\n",
    "        else:\n",
    "            t = torch.tensor([0], device=device, dtype=torch.long)\n",
    "            \n",
    "        t_emb = self.time_emb_layer(t)\n",
    "        return self.backbone(x_in, edge_index, edge_attr, t_emb, batch)\n",
    "\n",
    "# --- 5. M3 Wrapper (Diffusion) ---\n",
    "class M3_Physics_Diffusion(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_emb_layer = SinusoidalPositionalEmbeddings(dim=32)\n",
    "        self.backbone = UniversalGNN(in_channels=7, hidden_dim=hidden_dim, time_dim=32)\n",
    "        self.scheduler = DiffusionScheduler()\n",
    "\n",
    "    def forward(self, x_t, t, condition, edge_index, edge_attr, batch):\n",
    "        x_in = torch.cat([x_t, condition], dim=-1)\n",
    "        t_emb = self.time_emb_layer(t)\n",
    "        return self.backbone(x_in, edge_index, edge_attr, t_emb, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17bcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f3f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371834b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lvdn-diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
